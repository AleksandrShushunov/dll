{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport glob\n\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom IPython.display import clear_output\n%matplotlib inline\n\nsns.set(font_scale=1.2)\n# # sns.set_style(style='whitegrid')\n# device_num = 0\n# torch.cuda.set_device(device_num)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T19:44:27.113479Z","iopub.execute_input":"2023-10-01T19:44:27.113873Z","iopub.status.idle":"2023-10-01T19:44:32.937231Z","shell.execute_reply.started":"2023-10-01T19:44:27.113841Z","shell.execute_reply":"2023-10-01T19:44:32.936376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"code","source":"# разделим картинки на train и val в отношении 70 на 30 для каждого класса\ndata_dir = \"/kaggle/input/the-simpsons-characters-dataset/simpsons_dataset\"\ndata_image_paths = glob.glob(f\"{data_dir}/*/*.jpg\")\ndata_image_labels = [path.split('/')[-2] for path in data_image_paths]\ntrain_files_path, val_files_path = train_test_split(\n    data_image_paths, \n    test_size=0.3,\n    stratify=data_image_labels\n)\n\nprint(\n    f\"Изображений в train: {len(train_files_path)}\\nИзображений в val: {len(val_files_path)}\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:44:32.939162Z","iopub.execute_input":"2023-10-01T19:44:32.939657Z","iopub.status.idle":"2023-10-01T19:44:39.412136Z","shell.execute_reply.started":"2023-10-01T19:44:32.939625Z","shell.execute_reply":"2023-10-01T19:44:39.411236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 224\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.CenterCrop(input_size),\n    transforms.ColorJitter(0.5, 0.5, 0.5),\n    transforms.ToTensor(),\n])\n\n\nval_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.CenterCrop(input_size),\n    transforms.ToTensor(),\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=train_transform,\n)\n\nval_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=val_transform,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:44:39.413327Z","iopub.execute_input":"2023-10-01T19:44:39.413693Z","iopub.status.idle":"2023-10-01T19:45:05.152125Z","shell.execute_reply.started":"2023-10-01T19:44:39.413654Z","shell.execute_reply":"2023-10-01T19:45:05.151201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Количество классов в train: \", len(train_dataset.classes))\nprint(\"Количество классов в val: \", len(val_dataset.classes))\nprint(\"Количество классов одинаково: \", len(train_dataset.classes) == len(val_dataset.classes))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:05.154368Z","iopub.execute_input":"2023-10-01T19:45:05.154957Z","iopub.status.idle":"2023-10-01T19:45:05.162939Z","shell.execute_reply.started":"2023-10-01T19:45:05.154920Z","shell.execute_reply":"2023-10-01T19:45:05.161648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(style='white')\n\ndef show_images(dataset):\n    fig, ax = plt.subplots(\n        nrows=2, ncols=3, figsize=(8, 6),\n        sharey=True, sharex=True\n    )\n\n    for fig_x in ax.flatten():\n        idx = np.random.randint(low=0, high=6000)\n        img, label = dataset[idx]\n        fig_x.set_title(dataset.classes[label])\n        fig_x.imshow(img.numpy().transpose((1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:05.164104Z","iopub.execute_input":"2023-10-01T19:45:05.165125Z","iopub.status.idle":"2023-10-01T19:45:05.185604Z","shell.execute_reply.started":"2023-10-01T19:45:05.165093Z","shell.execute_reply":"2023-10-01T19:45:05.184654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:05.186870Z","iopub.execute_input":"2023-10-01T19:45:05.187971Z","iopub.status.idle":"2023-10-01T19:45:06.538813Z","shell.execute_reply.started":"2023-10-01T19:45:05.187939Z","shell.execute_reply":"2023-10-01T19:45:06.537787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:06.539786Z","iopub.execute_input":"2023-10-01T19:45:06.540089Z","iopub.status.idle":"2023-10-01T19:45:07.724746Z","shell.execute_reply.started":"2023-10-01T19:45:06.540062Z","shell.execute_reply":"2023-10-01T19:45:07.723901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.726190Z","iopub.execute_input":"2023-10-01T19:45:07.726744Z","iopub.status.idle":"2023-10-01T19:45:07.759070Z","shell.execute_reply.started":"2023-10-01T19:45:07.726710Z","shell.execute_reply":"2023-10-01T19:45:07.757965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves(history):\n    '''\n    Функция для вывода графиков лосса и метрики во время обучения.\n    '''\n    fig = plt.figure(figsize=(20, 7))\n\n    plt.subplot(1,2,1)\n    plt.title('Loss', fontsize=15)\n    plt.plot(history['loss']['train'], label='train')\n    plt.plot(history['loss']['val'], label='val')\n    plt.ylabel('loss', fontsize=15)\n    plt.xlabel('epoch', fontsize=15)\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.title('Accuracy', fontsize=15)\n    plt.plot(history['acc']['train'], label='train')\n    plt.plot(history['acc']['val'], label='val')\n    plt.ylabel('acuuracy', fontsize=15)\n    plt.xlabel('epoch', fontsize=15)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.761006Z","iopub.execute_input":"2023-10-01T19:45:07.761456Z","iopub.status.idle":"2023-10-01T19:45:07.769753Z","shell.execute_reply.started":"2023-10-01T19:45:07.761419Z","shell.execute_reply":"2023-10-01T19:45:07.769147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    model, \n    criterion,\n    optimizer, \n    train_batch_gen,\n    val_batch_gen,\n    num_epochs=10\n):\n    '''\n    Функция для обучения модели и вывода лосса и метрики во время обучения.\n    '''\n\n    history = defaultdict(lambda: defaultdict(list))\n\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        \n        start_time = time.time()\n\n        # устанавливаем поведение dropout / batch_norm  в обучение\n        model.train(True) \n\n        # на каждой \"эпохе\" делаем полный проход по данным\n        for X_batch, y_batch in train_batch_gen:\n            # обучаемся на текущем батче\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            \n            loss = criterion(logits, y_batch.long().to(device))\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            train_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            train_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        train_loss /= len(train_batch_gen)\n        train_acc /= len(train_batch_gen) \n        history['loss']['train'].append(train_loss)\n        history['acc']['train'].append(train_acc)\n    \n        # устанавливаем поведение dropout / batch_norm в режим тестирования\n        model.train(False) \n\n        # полностью проходим по валидационному датасету  \n        for X_batch, y_batch in val_batch_gen:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            loss = criterion(logits, y_batch.long().to(device))\n            val_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            val_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        val_loss /= len(val_batch_gen)\n        val_acc /= len(val_batch_gen) \n        history['loss']['val'].append(val_loss)\n        history['acc']['val'].append(val_acc)\n        \n        clear_output()\n\n        # печатаем результаты после каждой эпохи\n        print(\"Epoch {} of {} took {:.3f}s\".format(\n            epoch + 1, num_epochs, time.time() - start_time))\n        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n        print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_acc * 100))\n        \n        plot_learning_curves(history)\n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.774046Z","iopub.execute_input":"2023-10-01T19:45:07.774393Z","iopub.status.idle":"2023-10-01T19:45:07.786369Z","shell.execute_reply.started":"2023-10-01T19:45:07.774358Z","shell.execute_reply":"2023-10-01T19:45:07.785445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\n# не забудем перемешать train\ntrain_batch_gen = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\n# валидационный датасет мешать не нужно, а точнее бессмысленно\n# сеть на нём не обучается\nval_batch_gen = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.787798Z","iopub.execute_input":"2023-10-01T19:45:07.788106Z","iopub.status.idle":"2023-10-01T19:45:07.800799Z","shell.execute_reply.started":"2023-10-01T19:45:07.788072Z","shell.execute_reply":"2023-10-01T19:45:07.799836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Используем сеть MobileNet как`Feature Extractor`","metadata":{}},{"cell_type":"code","source":"from torchvision.models import mobilenet_v3_small,MobileNet_V3_Small_Weights","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.802409Z","iopub.execute_input":"2023-10-01T19:45:07.803063Z","iopub.status.idle":"2023-10-01T19:45:07.811843Z","shell.execute_reply.started":"2023-10-01T19:45:07.803026Z","shell.execute_reply":"2023-10-01T19:45:07.810783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# возьмём предобученную сеть\nfe_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n\n# заморозим все слои сети\nfor param in fe_model.parameters():\n    param.requires_grad = False\n\n# добавим над feature extractor сетью классификационный слой\nfe_model.classifier[3] = nn.Linear(1024, 43)\nfe_model = fe_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(fe_model.parameters(), lr=0.01)\n\nclf_model, history = train(\n    fe_model, criterion, optimizer, \n    train_batch_gen, val_batch_gen, \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:45:07.813016Z","iopub.execute_input":"2023-10-01T19:45:07.813614Z","iopub.status.idle":"2023-10-01T19:46:51.001370Z","shell.execute_reply.started":"2023-10-01T19:45:07.813575Z","shell.execute_reply":"2023-10-01T19:46:50.999910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:46:51.002391Z","iopub.status.idle":"2023-10-01T19:46:51.003415Z","shell.execute_reply.started":"2023-10-01T19:46:51.003156Z","shell.execute_reply":"2023-10-01T19:46:51.003180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Используем сеть как `FineTuning`","metadata":{}},{"cell_type":"code","source":"fine_tuning_model = nn.Sequential()\n\nfine_tuning_model.add_module('mobilenet', mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT))\n\n# добавим новые слои для классификации для нашей конкретной задачи\nfine_tuning_model.add_module('relu_1', nn.ReLU())\nfine_tuning_model.add_module('fc_1', nn.Linear(1000, 512))\nfine_tuning_model.add_module('relu_2', nn.ReLU())\nfine_tuning_model.add_module('fc_2', nn.Linear(512, 43))\n\nfine_tuning_model = fine_tuning_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(fine_tuning_model.parameters(), lr=0.01)\n\nclf_model, history = train(\n    fine_tuning_model, criterion, optimizer, \n    train_batch_gen, val_batch_gen, \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:46:51.004790Z","iopub.status.idle":"2023-10-01T19:46:51.005526Z","shell.execute_reply.started":"2023-10-01T19:46:51.005281Z","shell.execute_reply":"2023-10-01T19:46:51.005305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:46:51.006835Z","iopub.status.idle":"2023-10-01T19:46:51.007554Z","shell.execute_reply.started":"2023-10-01T19:46:51.007313Z","shell.execute_reply":"2023-10-01T19:46:51.007353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Добавим аугментацию изображений","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomCrop(128,padding=16),\n    transforms.ColorJitter(1, 1, 1),\n    transforms.RandomAffine([10,50]),\n    transforms.ToTensor(),\n])\n\n\nval_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.RandomCrop(128,padding=16),\n    transforms.ToTensor(),\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=train_transform,\n)\n\nval_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=val_transform,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:53:16.088034Z","iopub.execute_input":"2023-10-01T19:53:16.088421Z","iopub.status.idle":"2023-10-01T19:53:17.982805Z","shell.execute_reply.started":"2023-10-01T19:53:16.088387Z","shell.execute_reply":"2023-10-01T19:53:17.981825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:53:24.632697Z","iopub.execute_input":"2023-10-01T19:53:24.633033Z","iopub.status.idle":"2023-10-01T19:53:25.767501Z","shell.execute_reply.started":"2023-10-01T19:53:24.633004Z","shell.execute_reply":"2023-10-01T19:53:25.766618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:53:36.542778Z","iopub.execute_input":"2023-10-01T19:53:36.543115Z","iopub.status.idle":"2023-10-01T19:53:37.644298Z","shell.execute_reply.started":"2023-10-01T19:53:36.543075Z","shell.execute_reply":"2023-10-01T19:53:37.643464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_model = nn.Sequential()\n\naug_model.add_module('mobilenet', mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT))\n\n# добавим новые слои для классификации для нашей конкретной задачи\naug_model.add_module('relu_1', nn.ReLU())\naug_model.add_module('fc_1', nn.Linear(1000, 512))\naug_model.add_module('relu_2', nn.ReLU())\naug_model.add_module('fc_2', nn.Linear(512, 43))\n\naug_model = aug_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(aug_model.parameters(), lr=0.01)\n\nclf_model, history = train(\n    aug_model, criterion, optimizer, \n    train_batch_gen, val_batch_gen, \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:53:55.407458Z","iopub.execute_input":"2023-10-01T19:53:55.407786Z","iopub.status.idle":"2023-10-01T19:54:26.772302Z","shell.execute_reply.started":"2023-10-01T19:53:55.407759Z","shell.execute_reply":"2023-10-01T19:54:26.770875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Используем шедулеры для динамического изменения learning rate","metadata":{}},{"cell_type":"code","source":"input_size = 224\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.CenterCrop(input_size),\n    transforms.ColorJitter(0.5, 0.5, 0.5),\n    transforms.ToTensor(),\n])\n\n\nval_transform = transforms.Compose([\n    transforms.Resize(input_size),\n    transforms.CenterCrop(input_size),\n    transforms.ToTensor(),\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=train_transform,\n)\n\nval_dataset = torchvision.datasets.ImageFolder(\n    data_dir,\n    transform=val_transform,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:54:34.438418Z","iopub.execute_input":"2023-10-01T19:54:34.438763Z","iopub.status.idle":"2023-10-01T19:54:36.925772Z","shell.execute_reply.started":"2023-10-01T19:54:34.438735Z","shell.execute_reply":"2023-10-01T19:54:36.924797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    model, \n    criterion,\n    optimizer, \n    train_batch_gen,\n    val_batch_gen,\n    num_epochs=10\n):\n    '''\n    Функция для обучения модели и вывода лосса и метрики во время обучения.\n    '''\n\n    history = defaultdict(lambda: defaultdict(list))\n\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        \n        start_time = time.time()\n\n        # устанавливаем поведение dropout / batch_norm  в обучение\n        model.train(True) \n\n        # на каждой \"эпохе\" делаем полный проход по данным\n        for X_batch, y_batch in train_batch_gen:\n            # обучаемся на текущем батче\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            \n            loss = criterion(logits, y_batch.long().to(device))\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step() # добавляем шаг шедулера\n            \n            train_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            train_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        train_loss /= len(train_batch_gen)\n        train_acc /= len(train_batch_gen) \n        history['loss']['train'].append(train_loss)\n        history['acc']['train'].append(train_acc)\n    \n        # устанавливаем поведение dropout / batch_norm в режим тестирования\n        model.train(False) \n\n        # полностью проходим по валидационному датасету  \n        for X_batch, y_batch in val_batch_gen:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            loss = criterion(logits, y_batch.long().to(device))\n            val_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            val_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        val_loss /= len(val_batch_gen)\n        val_acc /= len(val_batch_gen) \n        history['loss']['val'].append(val_loss)\n        history['acc']['val'].append(val_acc)\n        \n        clear_output()\n\n        # печатаем результаты после каждой эпохи\n        print(\"Epoch {} of {} took {:.3f}s\".format(\n            epoch + 1, num_epochs, time.time() - start_time))\n        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n        print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_acc * 100))\n        \n        plot_learning_curves(history)\n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:54:40.420827Z","iopub.execute_input":"2023-10-01T19:54:40.421181Z","iopub.status.idle":"2023-10-01T19:54:40.436939Z","shell.execute_reply.started":"2023-10-01T19:54:40.421150Z","shell.execute_reply":"2023-10-01T19:54:40.432492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n\nfor param in exp_model.parameters():\n    param.requires_grad = False\n\nexp_model.classifier[3] = nn.Linear(1024, 43)\nexp_model = exp_model.to(device)\noptimizer = torch.optim.SGD(exp_model.parameters(), lr=0.02)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:58:18.324815Z","iopub.execute_input":"2023-10-01T19:58:18.325147Z","iopub.status.idle":"2023-10-01T19:58:18.412040Z","shell.execute_reply.started":"2023-10-01T19:58:18.325119Z","shell.execute_reply":"2023-10-01T19:58:18.411099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_model, history = train(\n    exp_model, criterion, optimizer, \n    train_batch_gen, val_batch_gen, \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:58:22.458009Z","iopub.execute_input":"2023-10-01T19:58:22.458357Z","iopub.status.idle":"2023-10-01T20:00:43.852443Z","shell.execute_reply.started":"2023-10-01T19:58:22.458310Z","shell.execute_reply":"2023-10-01T20:00:43.850934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    model, \n    criterion,\n    optimizer, \n    train_batch_gen,\n    val_batch_gen,\n    num_epochs=10\n):\n    '''\n    Функция для обучения модели и вывода лосса и метрики во время обучения.\n    '''\n\n    history = defaultdict(lambda: defaultdict(list))\n\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        \n        start_time = time.time()\n\n        # устанавливаем поведение dropout / batch_norm  в обучение\n        model.train(True) \n\n        # на каждой \"эпохе\" делаем полный проход по данным\n        for X_batch, y_batch in train_batch_gen:\n            # обучаемся на текущем батче\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            \n            loss = criterion(logits, y_batch.long().to(device))\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            \n            train_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            train_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        train_loss /= len(train_batch_gen)\n        train_acc /= len(train_batch_gen) \n        history['loss']['train'].append(train_loss)\n        history['acc']['train'].append(train_acc)\n    \n        # устанавливаем поведение dropout / batch_norm в режим тестирования\n        model.train(False) \n\n        # полностью проходим по валидационному датасету  \n        for X_batch, y_batch in val_batch_gen:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            logits = model(X_batch)\n            loss = criterion(logits, y_batch.long().to(device))\n            val_loss += np.sum(loss.detach().cpu().numpy())\n            y_pred = logits.max(1)[1].detach().cpu().numpy()\n            val_acc += np.mean(y_batch.cpu().numpy() == y_pred)\n\n        # подсчитываем лоссы и сохраням в \"историю\"\n        val_loss /= len(val_batch_gen)\n        val_acc /= len(val_batch_gen) \n        history['loss']['val'].append(val_loss)\n        history['acc']['val'].append(val_acc)\n        scheduler.step(val_loss) # Шаг шедулера в зависимости от лосса на валидации\n        \n        clear_output()\n\n        # печатаем результаты после каждой эпохи\n        print(\"Epoch {} of {} took {:.3f}s\".format(\n            epoch + 1, num_epochs, time.time() - start_time))\n        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n        print(\"  validation loss (in-iteration): \\t{:.6f}\".format(val_loss))\n        print(\"  training accuracy: \\t\\t\\t{:.2f} %\".format(train_acc * 100))\n        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_acc * 100))\n        \n        plot_learning_curves(history)\n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:04:51.533780Z","iopub.execute_input":"2023-10-01T20:04:51.534117Z","iopub.status.idle":"2023-10-01T20:04:51.544850Z","shell.execute_reply.started":"2023-10-01T20:04:51.534087Z","shell.execute_reply":"2023-10-01T20:04:51.543922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Plateau_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n\n# заморозим все слои сети\nfor param in Plateau_model.parameters():\n    param.requires_grad = False\n\n# добавим над feature extractor сетью классификационный слой\nPlateau_model.classifier[3] = nn.Linear(1024, 43)\nPlateau_model = Plateau_model.to(device)\noptimizer = torch.optim.SGD(Plateau_model.parameters(), lr=0.02)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:04:56.112633Z","iopub.execute_input":"2023-10-01T20:04:56.112960Z","iopub.status.idle":"2023-10-01T20:04:56.196330Z","shell.execute_reply.started":"2023-10-01T20:04:56.112931Z","shell.execute_reply":"2023-10-01T20:04:56.195396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_model, history = train(\n    Plateau_model, criterion, optimizer, \n    train_batch_gen, val_batch_gen, \n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:04:59.145708Z","iopub.execute_input":"2023-10-01T20:04:59.146034Z","iopub.status.idle":"2023-10-01T20:05:45.626544Z","shell.execute_reply.started":"2023-10-01T20:04:59.146005Z","shell.execute_reply":"2023-10-01T20:05:45.624992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}